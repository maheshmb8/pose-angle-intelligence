{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e72a08-6844-4333-b4fd-6a4aa413b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MBathija\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\MBathija\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MBathija\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MBathija\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MBathija\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# os.chdir(r'C:\\Users\\MBathija\\BI_Engineering\\Image_angle')\n",
    "import importlib\n",
    "import image_angle_func_updated\n",
    "importlib.reload(image_angle_func_updated)\n",
    "from image_angle_func_updated import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1501e97-343a-4178-8aef-b768bc0748bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████████████████████████████████████████████████████████████████| 14/14 [00:06<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " → output_dfnd_18012026093737.csv\n",
      " → output_dfnd_18012026093737.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:07<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " → clip_pca_dfnd_18012026093746.csv\n",
      " → clip_pca_dfnd_18012026093746.parquet\n",
      "Saved files:\n",
      " → final_df_18012026093746.csv\n",
      " → final_df_18012026093746.parquet\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load completely new / unseen test dataset\n",
    "# (No overlap with training data)\n",
    "# ==========================================\n",
    "new_data = pd.read_excel(r\"test_data.xlsx\")\n",
    "\n",
    "# ==========================================\n",
    "# Run MoveNet pose extraction on new images\n",
    "# ==========================================\n",
    "output_df_nd = run_movenet_multithread(\n",
    "    new_data,\n",
    "    url_col=\"image_url\",   # Image URL column\n",
    "    max_workers=7,         # Parallel workers\n",
    "    timeout=15             # Timeout per image (seconds)\n",
    ")\n",
    "\n",
    "# Save raw pose outputs for audit/debugging\n",
    "save_outputs(output_df_nd, \"output_dfnd\")\n",
    "\n",
    "# ==========================================\n",
    "# Generate CLIP embeddings for new images\n",
    "# ==========================================\n",
    "clip_embeddings_nd = run_clip_on_df(\n",
    "    new_data,\n",
    "    url_col=\"image_url\",\n",
    "    max_workers=7          # Keep low for torch stability\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# Apply previously trained PCA (NO refit)\n",
    "# ==========================================\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_vars = 32\n",
    "pca = joblib.load(\"pca_clip.pkl\")     # Load PCA fitted on training data\n",
    "\n",
    "# Transform new embeddings into PCA space\n",
    "clip_pca = pca.transform(clip_embeddings_nd)\n",
    "\n",
    "# ==========================================\n",
    "# Build PCA feature dataframe for new data\n",
    "# ==========================================\n",
    "clip_pca_df = pd.DataFrame(\n",
    "    clip_pca,\n",
    "    columns=[f\"clip_pca_{i}\" for i in range(pca_vars)]\n",
    ")\n",
    "\n",
    "# Attach metadata\n",
    "clip_pca_df[\"url\"] = new_data[\"image_url\"].values\n",
    "clip_pca_df[\"true_angle\"] = new_data[\"true_angle\"]\n",
    "\n",
    "# Save PCA-transformed features\n",
    "save_outputs(clip_pca_df, \"clip_pca_dfnd\")\n",
    "\n",
    "# ==========================================\n",
    "# Merge pose features with CLIP PCA features\n",
    "# (Final feature set for inference)\n",
    "# ==========================================\n",
    "final_df = output_df_nd.merge(\n",
    "    clip_pca_df,\n",
    "    on=\"url\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad096866-c46d-47ef-804c-2fd740498747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " → feature_df_18012026093751.csv\n",
      " → feature_df_18012026093751.parquet\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Build final model-ready feature dataframe\n",
    "# ==========================================\n",
    "feature_df = build_final_feature_df(\n",
    "    final_df     # Combined pose + CLIP feature dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c424eb21-e431-4682-8685-eb7dab69d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " → train_test_data_18012026093753.csv\n",
      " → train_test_data_18012026093753.parquet\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Attach labels / metadata back to features\n",
    "# ==========================================\n",
    "train_test_data = feature_df.merge(\n",
    "    final_df,       # Source containing labels & additional metadata\n",
    "    left_on=\"url\",  # Join key from feature dataframe\n",
    "    right_on=\"url\", # Join key from final dataframe\n",
    "    how=\"left\"      # Preserve all feature rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834ab138-9fb8-4d8b-ba6f-481596de5440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " → model_df_18012026093757.csv\n",
      " → model_df_18012026093757.parquet\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Filter rows with valid target labels\n",
    "# ==========================================\n",
    "model_df = (\n",
    "    train_test_data[\n",
    "        train_test_data[\"true_angle\"].notna()\n",
    "    ]\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Inspect dataset shape after filtering\n",
    "model_df.shape\n",
    "\n",
    "# Check class distribution of target variable\n",
    "model_df[\"true_angle\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca17d68e-1a8d-4e28-b96c-ac2447202200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 187) (14,)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Define non-feature (metadata / label) columns\n",
    "# ==========================================\n",
    "NON_FEATURE_COLS = [\"url\", \"true_angle\"]\n",
    "\n",
    "# ==========================================\n",
    "# Split features and target label\n",
    "# ==========================================\n",
    "X = model_df.drop(columns=NON_FEATURE_COLS)  # Model input features\n",
    "y = model_df[\"true_angle\"]                   # Target variable\n",
    "\n",
    "# Sanity check on dimensions\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca31610-ff79-4a6e-9ddd-f22fa088bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ==========================================\n",
    "# Load trained LightGBM pose classification model\n",
    "# ==========================================\n",
    "model = joblib.load(\"lgb_pose_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de97bc93-4ae8-497b-a9db-95f7dd903237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    B-tilted       0.00      0.00      0.00       0.0\n",
      "        Back       0.00      0.00      0.00       0.0\n",
      "       Front       0.00      0.00      0.00       0.0\n",
      "     UNKNOWN       0.00      0.00      0.00      14.0\n",
      "\n",
      "    accuracy                           0.00      14.0\n",
      "   macro avg       0.00      0.00      0.00      14.0\n",
      "weighted avg       0.00      0.00      0.00      14.0\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 2 10  2  0]]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Run inference on completely new / unseen images\n",
    "# (No train-test split, no data leakage)\n",
    "# ==========================================\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# ==========================================\n",
    "# Get prediction probabilities for confidence analysis\n",
    "# ==========================================\n",
    "y_prob = model.predict_proba(X)\n",
    "\n",
    "# ==========================================\n",
    "# Evaluate performance on unseen dataset\n",
    "# (This reflects real-world generalization)\n",
    "# ==========================================\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "\n",
    "# ==========================================\n",
    "# Build result dataframe for review & audit\n",
    "# ==========================================\n",
    "final_df = pd.DataFrame({\n",
    "    \"url\": model_df[\"url\"],          # Image identifier\n",
    "    \"actual\": model_df[\"true_angle\"],# Ground truth label\n",
    "    \"pred\": y_pred                   # Model prediction\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# Persist datasets for offline analysis\n",
    "# ==========================================\n",
    "model_df.to_excel(\"model_data_df.xlsx\", index=False)\n",
    "final_df.to_excel(\"final_predictions.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
